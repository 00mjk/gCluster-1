

## Directory Structure ##

    data
       +-<dataset>
          +-devices
          +-central
             +-devicesCells
             +-consolidate 
             +-results
             +-DBSCAN
          +-config
    script
    sumData
    getCluster

### data directory ###

This directory has data to be processed

#### <dataset\> ####

For organization, each dataset has its own directory named `<dataset>`

##### devices

In this directory resides datasets from devices (CSV files)

##### central

This directory is used to store all data in the central node. If it doesn't exist it will be created at run time

###### devicesCells

This directory receive cells sent by each device.

For testing purposes it could be have raw data.

###### consolidate

The script gather all csv cells data received on deviceCells directory and create a single file with all cells. 

It could have raw data if the devices sent raw data for testing purposes.

###### results

This directory stores the results of clusterization algorithms. 
- CSV Files
- SVG File if the clustering dataset has 2 dimensions.

###### DBSCAN

Files created by DBSCAN algorithm

##### config

Store CSV configuration files

## Running the experiment

All the scripts to run the experiment are stored on `script` directory.

The main script is `complete.py`, that run all phases of process.

Below the help screen:

    Options           Description
      -h              Show this help
      -d <dir>        The data files gathering by devices will be found on <dir>/devices directory
      -e <epsilon>    Value of epsilon: default = 10
      -m <cells>      Minimum Cells (default: 3)
      -f <force>      Minimum Force (default: 150)
      -r              Don't draw rectangles
      -g              Don't draw edges
      -p              Draw points
      -b              Draw numbers
      -x              Configuration file: default config.csv on the <dir> directory

First of all you need to create a new directory to store dataset in a CSV format.

The CSV files must be stored in the `devices` directory below `<dataset>` directory you've just created.

The script runs `sumData` for each CSV file simulating the process that runs at each device.

### Configuration file

You'll need a configuration file that by default will be found at `<dataset>/config` directory with name `config-<dataset>.csv`

This CSV file must be 4 lines:

1. Header: the names of variables. 
	- Example: `X,Y,Id,Classification`
2. Variable Identification. 
	- (**C**)lustered: variables to be clustered
	- (**N**)ot clustered
	- C(**L**)assification: The Ground Truth label (if exists) to test the clustering algorithm.
	- Example: `C,C,N,L`
3. Max Values. Values to be used on linear normalization. This line must contain the max value of each column. 
	- Example: `30,30,0,0` 
4. Min Values. Values to be used on linear normalization. This line must contain the min value of each column. 
	- Example: `2.9,3.7,0,0`

**Note:** All lines must have the same number of columns.

### sumData 

The program sumData is stored on `sumData/bin` directory and summarize data stored in `<dataset>/devices/*.csv` and create a new CSV file with cells created by summarization. 

- Input
	- `<dataset>/devices/*.csv`
- Outputs (`complete.py` script)
	- `<dataset>/central/devicesCells/cell-<dataset>-<seq>.csv`. Where `<seq>` is a sequential number.
	- `<dataset>/central/devicesCells/point-<dataset>-<seq>.csv`. **Note**: this file is generated if use `-p` option
- Parameter
    - `-e` Epsilon parameter. The default value for Epsilon is 10. 

### Consolidation

The script gather all output files created by `sumData` and create one single file.

- Inputs
	- `<dataset>/central/devicesCells/cell-<dataset>-<seq>.csv`. Where `<seq>` is a sequential number.
	- `<dataset>/central/devicesCells/point-<dataset>-<seq>.csv` if used `-p` option.
- Outputs
	- `<dataset>/central/consolidate/cells-<dataset>.csv`
	- `<dataset>/central/consolidate/cells-<dataset>.csv` if used `-p` option.

### Clustering

With the cells gathered consolidated in one single file, the clustering algorithm take place.

It´s important to notice that the outputs generated by `complete.py` script create different file names for different parameters. It´s useful to remember the parameters used when compare results.

The script create a prefix for output files that identify the Epsilon and the Force used to run the process.

The prefix has the format: `ennnfd.dddd` where `nnn` is the value of Epsilon and `d.dddd` is the value of force. Example: if Epsilon = 35 and Force = 0.0777 the prefix will be `e035f0.077`. 

- Inputs
	- `<dataset>/central/consolidate/cells-<dataset>.csv`
	- `<dataset>/central/consolidate/cells-<dataset>.csv` if used `-p` option.
- Outputs (`complete.py` script)
	- `<dataset>/central/results/<prefix>-cells-<dataset>.csv`.
	- `<dataset>/central/results/<prefix>-points-<dataset>.csv` if used `-p` option.
	- `<dataset>/central/results/<prefix>-points-<dataset>.svg`. If dimension = 2 it´s possible generate a plotting file (SVG), that could be opened in any browser.
- Clustering Parameters 
    - `-e` Epsilon parameter. The default value for Epsilon is 10. 
    - `-f` Force parameter. If the force between cells is greater or equal then parameter the cells become together.
    - `-m` Minimum Cells. If a cluster is formed by less than parameter, the cluster is discarded.
- Plotting Parameters: used to configure SVG output:
	- `-r` Don´t draw cells (rectangles) 
	- `-g` Don´t draw edges that link the cells creating the clusters
	- `-p` Draw points. Used to compare raw data with clustering results.
	- `-b` Draw Numbers. Draw the labels of clusters inside the cells and label of ground-truth (classification column on raw data) inside points.

#### Output formats

The output files are in the CSV format:

##### Cells

<table>
  <tr>
    <th>Field</th>
    <th>Description</th>
  </tr>
  <tr>
    <td>cell-id</td>
    <td>Sequential number<br></td>
  </tr>
  <tr>
    <td>number-points</td>
    <td>Same as cell-id<br></td>
  </tr>
  <tr>
    <td>CM-0</td>
    <td rowspan=4>Coordinates of center of mass<br></td>
  </tr>
  <tr>
    <td>CM-1</td>
  </tr>
  <tr>
    <td>...</td>
  </tr>
  <tr>
    <td>CM-n</td>
  </tr>
  <tr>
    <td>qty-cells-cluster</td>
    <td>Quantity of cells of the cluster <br></td>
  </tr>
  <tr>
    <td>gGluster-label</td>
    <td>Label of Cluster defined by gCluster<br></td>
  </tr>
  <tr>
    <td>ground-truth-label<br></td>
    <td>Label of Ground Truth cluster (Class Column)<br></td>
  </tr>
</table>


##### Points
<table>
  <tr>
    <th>Field</th>
    <th>Description</th>
  </tr>
  <tr>
    <td>Coord-0</td>
    <td rowspan="4">Coordinates of point<br></td>
  </tr>
  <tr>
    <td>Coord-1</td>
  </tr>
  <tr>
    <td>...</td>
  </tr>
  <tr>
    <td>Coord-n</td>
  </tr>
  <tr>
    <td>gGluster-label</td>
    <td>Label of Cluster defined by gCluster<br></td>
  </tr>
  <tr>
    <td>ground-truth-label<br></td>
    <td>Label of Ground Truth (Class. Column)<br></td>
  </tr>
</table>
 
## DBSCAN

The script `DBSCAN.py` runs the DBSCAN clustering algorithm to compare results.

It´s possible run DBSCAN over the points (raw data) or over the center of mass of cells generated by `sumData` program.

The goal is compare gCluster Algorithm with DBSCAN in two situations:
1. Run DBSCAN over the all data to compare centralized data and distributed data approaches.
2. Run DBSCAN over cell´s center of masses to verify if a conventional and mature algorithm has good performance over summarized data.

        Options         Description
        -h              Show this help
        -d <dir>        Directory of files
        -pr <pre>       Prefix of files (e<epsilon>f<force (with 4 decimals)> - Ex. e014f0.1500)
        -t <opt>        <opt> = c or p (for cells or points respectively)
        -e <value>      Epsilon value
        -m <value>      Min points value
        -l              Print legend

- Options
	- `-d` is the `<dataset>` directory
	- `-t` type of data: c for cells (center of mass of cells) or p for points (raw data).
	- `-pr` Prefix of files. Is the best way to ensure you are comparing correctly. The script uses this prefix to find out the input file
- DBSCAN parameters
	- `-e` Epsilon value. It´s important to notice that due the normalization the distance between minimum and maximum values for all dimensions is one. So this is a good reference to choose a good value of Epsilon.
- Inputs
	- `<dataset>/central/results/<prefix>-cells-<dataset>.csv` for cells (option `-t c`)
	- `<dataset>/central/results/<prefix>-points-<dataset>.csv` for points (option `-t p`) 
- Outputs
	- `<dataset>/central/DBSCAN/<prefix>-result-cells-<dataset>.csv` for cells (option `-t c`)
	- `<dataset>/central/DBSCAN/<prefix>-result-points-<dataset>.csv` for points (option `-t p`)

### Output formats


<table>
  <tr>
    <th>Field</th>
    <th>Description</th>
  </tr>
  <tr>
    <td>CM-0</td>
    <td rowspan=4>Coordinates of point or cell<br></td>
  </tr>
  <tr>
    <td>CM-1</td>
  </tr>
  <tr>
    <td>...</td>
  </tr>
  <tr>
    <td>CM-n</td>
  </tr>
  <tr>
    <td>gGluster-label</td>
    <td>Label of Cluster defined by DBSCAN<br></td>
  </tr>
  <tr>
    <td>ground-truth-label<br></td>
    <td>Label of Ground Truth cluster (Class Column)<br></td>
  </tr>
</table>

